\documentclass[]{article}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{ulem}
\usepackage{verbatim}
\usepackage{url}
\linespread{1.3}
\usepackage{graphicx}

% Title Page
\title{Exploring classification tools in sklearn}
\author{M. Hassell}


\begin{document}
\maketitle

\section{Introduction}

In the context of machine learning, {\it classification problems} ask for a model to assign to some input a discrete class label.  This label could be anything from a 0 or 1 (perhaps corresponding to a yes or no answer to some question), a color, or a digit representing a numeral, as is the case with the MNIST data set we will explore here.  Classification differs from another major ML strategy, {\it regression} whereby a (generally) continuous predictor is assigned to some data.  This could be something like to price of a house given the number of bedrooms, bathrooms, and zip code.  One could also predict the amount of electricity that needs to be generated at a given time of the day for a certain area, given the season, population, predicted weather, and historical usage.  We will focus for now on classification problems, where our desired outcomes, or {\it labels} take values in a discrete set.

\section{Jumping in}

We'll start by using theMNIST handwriting data that is included with {\tt sklearn} in the {\tt datasets} module.  This is convenient since the data has already been acquired and put into a standardized format, which is no small task on its own.   The data set includes 1797 images of the digits 0-9, stored as $8 \times 8$ arrays with pixel intensities 0-255.  When we perform any model training or prediction on elements of this data set, we will need to reshape these arrays into $64 \times 1$ vectors, as elements of $\mathbb{R}^{64}$.   Our first attempt at training a model for this dataset will make use of a support vector classifier (we'll see this as SVC within the sklearn environment).

\section{What's it all mean?}

A support vector classifier for {\it two} classes seeks to find a separating hyperplane (if possible) that separates the two classes with the greatest possible margin (check the Wikipedia for some nice pictures of this).  Supposing that we are in such a case, the equations of the so-called {\it maximum margin separating hyperplane} can be expressed by the following linear equations
$$
Equations~here
$$

{\it discussion of the equations}

\section{A natural generalization}

Now that we've understood the case of two classes in the linearly separable case, we can look into the two natural generalizations.  The first obvious generalization is to consider more than two classes.  What if we have $n$ distinct classes that are linearly separable.  We can imagine this like one of the variety pack cheesecakes from Barnes and Noble.  There are eight slices of cheesecake and four potential flavors, each occupying one quarter of the cheesecake.  We seek to divide the cheesecake into quarters, where each quarter contains two slices 

\end{document}          
